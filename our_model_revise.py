# -*- coding: utf-8 -*-
"""our_model_revise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6lIcajty5XcNoUlssI6Nc60XQ8RQc_f
"""

# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab
# login with your google account and type authorization code to mount on your google drive.
import os
from google.colab import drive
drive.mount('/content/drive')

pip install efficientnet_pytorch

## 학습 코드
import numpy as np
import json
import PIL
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torchvision import transforms, utils
from skimage import io, transform
import matplotlib.pyplot as plt
import time
import os
import copy
import random
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from efficientnet_pytorch import EfficientNet

device = 'cuda' if torch.cuda.is_available() else 'cpu'

class Mydataset(Dataset):
    def __init__(self,csvfile,root_dir,transform=None):
        self.landmarks_frame=pd.read_csv(csvfile)
        self.root_dir=root_dir
        self.transform=transform
        self.diagnose_dict = {"melanoma": 0, "nevus": 1}

    def __len__(self):
        return len(self.landmarks_frame)

    def one_hot_sex(self, sex):
        if sex == "male":
            return [1, 0]
        else:
            return [0, 1]

    def one_hot_age(self, age):
        arr = [0] * 10
        arr[int(age/10)] = 1
        return arr

    def one_hot_site(self, site):
        if site == "head/neck":
            return [1, 0, 0, 0, 0, 0]
        elif site == "upper extremity":
            return [0, 1, 0, 0, 0, 0]
        elif site == "lower extremity":
            return [0, 0, 1, 0, 0, 0]
        elif site == "torso":
            return [0, 0, 0, 1, 0, 0]
        elif site == "palms/soles":
            return [0, 0, 0, 0, 1, 0]
        elif site == "oral/genital":
            return [0, 0, 0, 0, 0, 1]
        else: assert(0)

    def __getitem__(self,idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[idx, 0])
        image = io.imread(img_name+".jpg")
        image = transforms.ToTensor()(image)
        if self.transform:
            image = self.transform(image)

        # sample = image
        try:
            diagnose = self.diagnose_dict[self.landmarks_frame.iloc[idx, 5]]
        except:
            diagnose = 1

        sex = self.landmarks_frame.iloc[idx, 2]
        sex = self.one_hot_sex(sex)
        
        age = self.landmarks_frame.iloc[idx, 3]
        age = self.one_hot_age(age)

        site = self.landmarks_frame.iloc[idx, 4]
        site = self.one_hot_site(site)

        metadata = np.array(sex + age + site).astype(np.float32)
        metadata = torch.from_numpy(metadata)

        image = image.to(device)
        metadata = metadata.to(device)
        diagnose = torch.tensor(diagnose, dtype=torch.long)
        diagnose = diagnose.to(device)
        
        # landmarks = landmarks.astype('float').reshape(-1, 2)
        sample = {'image': image, 'metadata': metadata, 'diagnose': diagnose}
        # sample = np.array([image, image])
        

        return sample



# Define train/test data loaders  
# Use data augmentation in training set to mitigate overfitting. 
# train_transform = transforms.Compose([
#     transforms.RandomCrop(32, padding=4),
#     transforms.RandomHorizontalFlip(),                                
#     transforms.ToTensor(),
#     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))
#     ])

train_transform = transforms.Compose([
    transforms.Resize((240, 240))
    ])


test_transform = transforms.Compose([
    transforms.Resize((224, 224))
    ])
# test_transform = transforms.Compose([                       
#     transforms.ToTensor(),
#     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))
#     ])



train_dataset = Mydataset(csvfile='/content/drive/My Drive/2020Fall/인공개 팀플/jpeg-melanoma-256x256/csv/train_no_unknown.csv',root_dir='/content/drive/My Drive/2020Fall/인공개 팀플/jpeg-melanoma-256x256/train',transform=train_transform)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)
# test_dataset = Mydataset(csvfile='/content/drive/My Drive/2020Fall/인공개 팀플/jpeg-melanoma-256x256/csv/data_no_unknown_test.csv',root_dir='/content/drive/My Drive/2020Fall/인공개 팀플/jpeg-melanoma-256x256/train',transform=test_transform)
# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, drop_last=True)
#test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)



# print(type(train_dataset[0]['diagnose']))
# print(train_dataset[0]['diagnose'])
# print(train_dataset[0]['metadata'])

class MyNetwork(nn.Module):
    def __init__(self):
        super(MyNetwork, self).__init__()
        cnn_model_name = 'efficientnet-b1'
        self.cnn_model = EfficientNet.from_pretrained(cnn_model_name, num_classes=2).to(device)
        print(EfficientNet.get_image_size(cnn_model_name))
        self.cnn = nn.Sequential (
            nn.Linear(1280, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(p=0.3),
        )

        self.meta = nn.Sequential (
            nn.Linear(18, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(128, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(128, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(p=0.3),
        ) 

        self.post = nn.Sequential (
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(128, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(32, 2),
            nn.BatchNorm1d(2),
            nn.ReLU(),
            nn.Dropout(p=0.3),
        )

        ################################

    def forward(self, image, metadata):
        image = self.cnn_model.extract_features(image)
        
        image = nn.AdaptiveAvgPool2d(output_size=(1, 1))(image)
        image = torch.squeeze(image, -1)
        image = torch.squeeze(image, -1)
        img_out = self.cnn(image)
        meta_out = self.meta(metadata)
        output = self.post(torch.cat((img_out, meta_out), dim = 1))
        #######################################################################
        return output

model = MyNetwork().to(device)
print(sum(p.numel() for p in model.parameters() if p.requires_grad))



learning_rate = 0.005
epoch = 100
training_loss = 0
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
start_time = time.time()
print("시작")
for ep in range(epoch):
    training_loss = 0
    training_accuracy = 0
    train_num_data = 0
    model.train()
    for batch_idx, dictionary in enumerate(train_dataloader):
        #  Send `x` and `y` to either cpu or gpu using `device` variable.
        x = dictionary['image']
        # print(x.size())
        # x = x.to(device=device)

        y = dictionary['diagnose']
        # print(y.size())
        meta = dictionary['metadata']
        print(meta)
        # # Feed `x` into the network, get an output, and keep it in a variable called `logit`. 
        logit = model(x, meta)
        # print(logit[:10])
        # print(y[:10])
        accuracy = (logit.argmax(dim=1) == y).float().mean()
        loss = nn.CrossEntropyLoss()(logit, y)
        optimizer.zero_grad()
        # backward the computed loss. 
        loss.backward()
        # update the network weights. 
        optimizer.step()

        training_loss += loss.item()*x.shape[0]
        training_accuracy += accuracy.item()*x.shape[0]
        train_num_data += x.shape[0]

        # if (batch_idx % 10 == 0):
            # print(time.time() - start_time)
            # print(loss)
            # print(batch_idx)



    training_loss /= train_num_data
    training_accuracy /= train_num_data
    print(f'training_loss : {training_loss:.3f}')
    print(f'training_accuracy : {training_accuracy:.3f}')
    
    # model.eval()
    # with torch.no_grad():
    #     test_loss = 0.
    #     test_accuracy = 0.
    #     test_num_data = 0.
    #     for batch_idx, dictionary in enumerate(test_dataloader):
    #         # Send `x` and `y` to either cpu or gpu using `device` variable..
    #         x = dictionary['image']
    #         y = dictionary['diagnose']
    #         meta = dictionary['metadata']

    #         # Feed `x` into the network, get an output, and keep it in a variable called `logit`.
    #         logit = model(x, meta)
    #         # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.
    #         loss = nn.CrossEntropyLoss()(logit, y)

    #         # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.
    #         accuracy = (logit.argmax(dim=1) == y).float().mean()

    #         test_loss += loss.item()*x.shape[0]
    #         test_accuracy += accuracy.item()*x.shape[0]
    #         test_num_data += x.shape[0]

    #     test_loss /= test_num_data
    #     test_accuracy /= test_num_data
    #     print(f'test_loss : {test_loss:.3f}')
    #     print(f'test_accuracy : {test_accuracy:.3f}')

torch.save(model.state_dict(), f'/content/drive/My Drive/2020Fall/인공개 팀플/results/fourth_model.pt')
print("model saved")
